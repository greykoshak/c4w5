<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Классификация изображений на Tensorflow</title>
</head>
<body>
<div><p>
    В этом видео мы с вами продолжим знакомство с framework Tensorflow и обсудим, как с помощью этой библиотеки решать задачу классификации изображений с помощью нейронной сети. Давайте импортируем tensorflow, matplotlib и загрузим dataset mnist. Mnist представляет из себя набор рукописных номеров от 0 до 9. Именно их мы и будем классифицировать. Давайте посмотрим как выглядят наши данные. Это просто картинки 28x28 черно-белые. Мы будем пытаться их распознавать с помощью нейронной сети. Посмотрим на размер из наших данных, в качестве данных в качестве x у нас просто вектор 784. То есть как раз картинка 28x28 и с каких-то цифр, а в качестве меток или лейблов у нас будет onehot вектор из десяти значений. То есть давайте посмотрим как это выглядит. У нас все нули кроме одной единички, которая как раз соответствует нашей метки. Но, а X это просто у нас числа от 0 до 1-го соответствующее какому-то значению в grasescrill. Давайте определим некоторые параметры нашей модели. У нас будет соответствующая learning_rate, у нас будет 1000 эпох и batch_size это сколько будет передавать объектов в единицу обучения нашей нейронной сети. На первом скрытом слое нашей нейронной сети у нас будет 256 нейронов, на втором скрытом слое тоже 256. То есть у нас будет нейронная сеть из двух скрытых слоев. На входе у нас вектор из 784 значений, как мы уже видели. То есть у нас картинка 28х28 передается просто как один вектор. Всего у нас 10 классов, то есть от 0 до 9 и мы будем решать задачу классификации. То есть, у нас на выходе нашей нейронной сети будет 10 чисел в соответствии каждой метки, каждому классу мы будем распределять какую-то уверенность. То есть будем решать задачу софт макс логистической регрессии. Давайте определим два placeholder для наших данных X и для меток Y. Данные у нас размерности 784, будем передавать какое-то количество объектов в наш placeholder. Ну, и классов у нас 10, как мы уже с вами поговорили. Давайте определим параметры нашей нейронной сети, ее конфигурацию. Как вы знаете нейронная сеть представляет из себя какое-то количество нейронов, которые передают друг-другу информацию. Один нейрон обладает набором весов и получая информацию на вход он приумножает X полученный с весами и прибавляет biases turn в данном случае B и отправляет это все дальше. Итак, на первом скрытом слое мы случайно инициализируем наши веса. У нас размерность будет соответствует нашему входу и количеству нейронов. Именно такой у нас будет матрица наших весов. На втором слое. Второй слой у нас принимает в качестве входа первый слой, у нас количество нейронов будет n_hidden_2. На выходе нашей нейронной сети мы тоже инициализируем здесь веса, случайно, выход у нас принимает все нейроны с второго слоя и возвращает распределение над классами - у нас 10 классов. И каждому весу у нас должен быть какой-то biasesturn. Тоже определим соответствующие переменные, тоже их случайно инициализируем. У нас будет по biasesturn на каждый нейрон в нашем скрытом слое 1, скрытом слое 2 и на каждый класс в нашем выходе. Таким образом мы определили конфигурацию нашей сети - наши веса. Мы их случайно инициализировали и потом наша нейронная сеть при обучении будет их как-то изменять, чтобы получать корректные предсказания. Давайте определим функцию network, которая будет в нашем вычислительном графе как раз оперировать. Наша функция network принимает наши данные и отправляет их в первый слой. Что же у нас происходит в первом слое? Как вы знаете, в первую очередь мы перемножаем наши входные данные с весами. То есть мы перемножаем в первом слое полученный X с весами скрытого слоя номер один и добавляем полученные произведение, добавляем к нему biasesturn, то есть все наши biase. Делаем это в тензорной форме и отправляем все дальше во второй слой. Таким образом, наш второй слой, получает информацию с первого слоя, получает информацию от всех нейронов, перемножает их уже со своими весами и добавляет конечно же biasesturn. Ну и, выходной слой у нас принимает все значения с предыдущего, то есть со второго слоя, перемножает их со своими весами и делает то же самое, добавляет biasesturn. И мы возвращаем из нашей сети как раз тот самый output_layer. Определить такую функцию, именно она и будет нашей нейронной сетью. Именно ее мы будем обучать. Однако, как же мы ее будем обучать? Нам нужна какая-то функция потерь. Давайте ее определим! А наша нейронная сеть возвращает какое-то распределение над классами. То есть лоджиты. Давайте определим нашу функцию потерь. Мы будем здесь минимизировать кросс-энтропию, потому что мы будем решать задачу логистической регрессии. То есть мы будем пытаться сделать так, чтобы наше распределение над классами, о котором мы предсказали, максимально соответствовало реальному распределению, то есть нашему onehot вектору. Мы передаем лейблы, передаем лоджиты, то есть полученные распределение над классами в нашу функцию, которая вычисляет кросс-энтропию и пытаемся ее минимизировать. Все это тоже оператор в нашем вычислительном графе. И поэтому они будут выпускаться позже, когда мы будем запускать нашу сессию. В качестве оптимайзера, то есть в качестве оптимизационного алгоритма мы будем использовать Adam. Зададим learn_create, который мы указали выше и будем минимизировать с помощью нашего оптимизационного алгоритма функцию потерь, которую мы задали. А дальше, мы будем минимизировать какую-то метрику. В данном случае мы будем минимизировать метрику accuracy. Приведем наши предсказания к меткам, которые у нас должны быть и будем считать accuracy в таком виде. Не обязательно сейчас углубляться в то, что значат соответствующего argmax и equal функции. Equal просто считать сколько у нас попало значения. Argmax находит тот самый лоджит, который у нас максимален. То есть какой класс наша нейронная сеть передает на выход. Допустим, если наш лоджит один из лоджитов будет 0,8, значит мы с уверенностью 0,8 говорим, что это, например, - 2. Именно его и найдет функция argmax, ну и мы будем считать, что это именно это предсказание. Посмотрим сколько предсказаний у нас корректных и посчитаем accuracy. Давайте запустим эту ячейку, инициализируем все перемены и начнем собственно обучать нашу нейронную сеть. Конечно, мы инициализируем сессию, инициализируем все наши переменные со случайными значениями, начальными, например, variable и будем в цикле обучать нашу нейронную сеть по эпохам. Таким образом у нас будет цикл по эпохам, у нас будет 1000 эпох, как мы помним, в каждой эпохе у нас будет происходить одинаковая операция. Для начала, для того чтобы получить наши batch, на которых мы будем обучать нашу нейронную сеть, воспользуемся стандартным методом next_batch, который принимает количество объектов на которых мы хотим обучать нашу нейронную сеть. Мы здесь получим 128 X, 128 Y и отправим их в тренировку, а тренировка с помощью сессии. В данном случае у нас train переменная отвечает за оптимизационную функцию. Вот наш оптимайзер, он минимизирует функцию потерь, которая задана и для того, чтобы наша функция оптимизации работала, для того чтобы наш граф запустить, нам нужно передать конечное значение X и Y, наши параметры feed_dict. Именно это мы и сделаем передадим batch_x и batch_y и раз в 50 эпох будем выводить отладочную информацию, будем смотреть как у нас выглядит наши accuracy. Как же мы это делаем? Для того, чтобы получить какую-нибудь информацию из нашего графа, мы должны запустить sess_run, то есть запустить нашу сессию и указать ту самую ноду, вершину, которую мы хотим вычислить и передать соответствующие параметры. Здесь мы будем смотреть accuracy на всем trainset, то есть смотреть как хорошо мы умеем предсказывать на текущем trainset и выведем эту информацию. В конце мы посчитаем итоговую accuracy, уже на тестовом сайте. То есть mnist dataset наш разбит на тренировочную и тестовую выборку. Мы обучаемся на тренировочной и в конце посмотрим нашу accuracy на тестовых объектах. Давайте запустим наше обучение. У нас пошло обучение. Давайте пока пробежимся еще раз, что здесь происходит. У нас в цикле по эпохам. Мы бежим и каждый раз делаем одно и то же. Мы достаем следующий batch, то есть следующий набор объектов, отправляем их в наш вычислительный граф. Вычислительный граф у нас тренируется с помощью объекта train, объект train, соответствует оптимизатору adam_optimaizer, который минимизирует функцию потерь. Для того, чтобы нашу функцию потерь минимизировать, мы должны передать наши данные, прокинуть их сквозь наш вычислительный граф и получить какое-то значение. Чтобы посмотреть как хорошо наш вычислительный граф, наша модель работает, мы вычисляем метрику accuracy и передаем ее тоже в запуск сессии. Посмотрим, обучалась ли наша модель. Да! Как видите у нас тестовая accuracy 80%, что достаточно неплохо. В принципе с другой инициализацией, если бы мы перезапустили бы наше обучение нейронной сети, скорее всего мы могли бы получить даже больше. Однако, и для начала это уже неплохо. Мы построили с вами нейронную сеть, которая просто принимает на вход картинки и с accuracy 80%. В 80% случаях корректно угадывает label, достаточно неплохо. Существует, также дополнительная надстройка на Tensorflow, которая называется Keras. Keras изначально был отдельный библиотекой, однако сейчас он встроен в Tensorflow, позволяет производить операции над графами Tensorflow в более упрощенном виде, то есть в более простом синтаксисе. Очень часто это бывает полезно. Keras появился в версии 1.4 и если у вас версии Tensorflow больше, вы можете использовать эту настройку, для того, чтобы чуть-чуть попроще строить базовые нейронные сети. Давайте посмотрим как это делается. Будем решать ту же самую задачу, тоже будем работать с mnist и dataset тоже есть в Keras. Мы здесь загрузим наши данные, немного их преобразуем, можно не вдаваясь в подробности, что именно здесь происходит, мы просто преобразуем наши данные и будем работать с ними точно так же, как работали с предыдущей задачей. Однако, обучение нейронной сети и то, как мы специфицируем ее конфигурацию, ее архитектуру, здесь немного отличается. Мы определяем модель Sequential, то есть последовательную модель. У нас наша нейронная сеть будет последовательностью определенных слоев, как собственно все нервные сети. Первым слоем у нас будет Dense слой, то есть слой просто из нейронов, которых 512. Функций активации в нашем нейронном слое будет reloop. Опять же у нас есть какой-то набор нейронов, каждый из нейронов у нас принимает на вход данные, то есть данные это X. Он перемножает их со своими весами, прибавляет biasesturn и отправляет функцию активации. Именно это является выходом нейронов. Здесь у нас функция активации это reloop. Размерность у нас все тоже 784. Мы принимаем вектор наших значений и соответствующие картинки 28 x 28. Дальше, мы добавляем в нашу модель следующий слой и слой у нас здесь будет Dropout. Dropout случайным образом во время тренировки выкидывает определенные нейроны, чтобы наши нейроны были меньше скоррелированы друг с другом и наша модель выучила более корректные природные зависимости. Добавляем еще один слой, точно так же, добавляем еще один Dropout. И как вы видите мы просто последовательно добавляем какие-то слои. Мы не определяем функцию network, мы не пишем какой-то класс. Мы просто посредством добавляем уже описанные слои из нашей библиотеке и часто это бывает удобно, потому что мы просто хотим построить как лего, какую-то определенную архитектуру. И в конце у нас будет опять же Dense слой, то есть на выходе у нас, выход будут принимать все нейроны с предыдущего слоя и на выходе у нас будет, выход размерности 10 опять же и с функций активации softmax, потому что нам нужно распределение над нашими классами, мы решаем задачу логистической регрессии. Давайте запустим нашу модель и для того чтобы ее скомпилировать и запустить нам уже не нужно писать какие-то циклы, нам не нужно инициализировать сессии, все достаточно просто, все намного проще. Для начала скомпилируем нашу модель, то есть зададим параметры этой модели и сохраним ее. Мы будем минимизировать кросс-энтропию, как и делали предыдущий раз. Однако, делается это немного проще, мы просто задаем следующий параметр. Будем использовать оптимизатор Adam, как и прошлый раз и будем оптимизировать метрику accuracy. Мы сконфигурировали нашу модель, теперь мы можем ее запустить. Здесь у нас такой же интерфейс, как уже в знакомом вам sklearn, мы делаем feed. Feed на train, то есть мы передаем X Y, передаем соответствующие batch_size, который встречался раньше. Будем тренироваться тоже 1000 эпох и валидироваться на X тесте, Y тесте. Обратите внимание, что здесь мы видим уже интерфейс, который позволяет нам в более приятной форме смотреть на прогресс. И в принципе Keras, это как раз та самая библиотека, которая вам позволит строить базовые модели, чуть более просто чем Tanserflow. Однако, Tanserflow намного более гибкий и позволит вам чуть глубже погрузиться в архитектуру нейронной сети и изменить определенные моменты надстроить, написать свои собственные какие-то функции активации и так далее. Как видите наша модель обучилась. Давайте посмотрим на accuracy на тесте. Тестовая accuracy получилось здесь намного лучше 0,97. Потому что, у нас чуть более сложная нейронная сеть, ну и возможно мы чуть лучше инициализировали параметры. Итак, в этом видео мы с вами коротко поговорили о том, как же с помощью Tanserflow обучать свою собственную нейронную сеть и решили стандартную классическую "Hallo world" задачу про нейронные сети. Квалификацию рукописных номеров mnist. Теперь вы умеете применять Tanserflow и познакомились с настойкой на Tanserflow Keras, которая вам позволит в упрощенном виде создавать нейронные сети, например, в последовательном варианте. Удачи!
</p></div>
</body>
</html>